# ğŸ“ ê³ ê¸‰ ê·¸ë˜í”„ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ì´ë“œ
## ì‚¬ì§„ë³„ ìƒì„¸ ë¶„ì„ ë° ì‹¤í–‰ ë°©ë²•

---

## ğŸ“¸ ì‚¬ì§„ 1: Support Vector Machine (SVM)

### ğŸ“‹ ì´ ê·¸ë˜í”„ê°€ ë­”ê°€ìš”?
- **ëª©ì **: SVM ë¶„ë¥˜ê¸°ì˜ ê²°ì • ê²½ê³„(Decision Boundary)ë¥¼ ì‹œê°í™”
- **ì™¼ìª½ ì´ë¯¸ì§€**: 2D í‰ë©´ì—ì„œ ë‘ í´ë˜ìŠ¤ë¥¼ ë¶„ë¦¬í•˜ëŠ” ìµœì ì˜ ì´ˆí‰ë©´(hyperplane)
  - ê²€ì€ ì : í´ë˜ìŠ¤ 1 (Loan)
  - í° ì : í´ë˜ìŠ¤ 0 (No Loan)
  - 3ê°œ ì„ : ë‹¤ì–‘í•œ ë¶„ë¦¬ ì˜µì…˜ (ì´ˆë¡ìƒ‰ì´ ìµœì )

- **ì˜¤ë¥¸ìª½ ì½”ë“œ**: SVM ëª¨ë¸ í•™ìŠµ ë° ì •ê·œí™”(Normalization)
  - í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ [0, 2.0] ë²”ìœ„ë¡œ ì •ê·œí™”
  - SVM ëª¨ë¸ ìƒì„± ë° í•™ìŠµ ì‹œê°„ ì¸¡ì •

### ğŸ” ìš°ë¦¬ í”„ë¡œì íŠ¸ì— ì ìš© ê°€ëŠ¥?
| í•­ëª© | ìƒíƒœ | ì´ìœ  |
|------|------|------|
| **í•„ìš”ì„±** | â­â­â­ (ë†’ìŒ) | ìš°ë¦¬ëŠ” í˜„ì¬ XGBoost ëª¨ë¸ ì‚¬ìš© ì¤‘ |
| **ì¶”ì²œ** | âœ… ë¹„êµ ëª¨ë¸ë¡œ ì¶”ê°€ | SVMë„ ì¢‹ì€ ë¹„êµ ëŒ€ìƒ |
| **ìš°ì„ ìˆœìœ„** | 5ìˆœìœ„ | ì´ë¯¸ 3ê°œ ëª¨ë¸ ë¹„êµ ì™„ë£Œ |

### ğŸ’» ì§ì ‘ ë§Œë“œëŠ” ë°©ë²•

**íŒŒì¼ëª…**: `svm_decision_boundary.py` ìƒì„±

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.preprocessing import MinMaxScaler
import pandas as pd

# 1. ë°ì´í„° ë¡œë“œ
train_data = pd.read_csv('cs-training-engineered.csv')
test_data = pd.read_csv('cs-test-engineered.csv')

# 2. X, y ë¶„ë¦¬ (ë§ˆì§€ë§‰ ì»¬ëŸ¼ì´ target)
trainX = train_data.iloc[:, :-1].values
trainY = train_data.iloc[:, -1].values
testX = test_data.iloc[:, :-1].values
testY = test_data.iloc[:, -1].values

# 3. ì •ê·œí™” (SVMì€ ì •ê·œí™” í•„ìˆ˜)
scaler = MinMaxScaler(feature_range=(0, 2))
trainX = scaler.fit_transform(trainX)
testX = scaler.transform(testX)

# 4. SVM ëª¨ë¸ ìƒì„±
svm_model = SVC(kernel='rbf', C=1.0, gamma='scale')
svm_model.fit(trainX, trainY)

# 5. 2ê°œ íŠ¹ì„±ë§Œ ì„ íƒí•´ì„œ ì‹œê°í™” (ì²˜ìŒ 2ê°œ íŠ¹ì„±)
X_2d = trainX[:, :2]  # ì²« 2ê°œ íŠ¹ì„±ë§Œ
y = trainY

# 6. ë©”ì‹œ ê·¸ë¦¬ë“œ ìƒì„±
h = 0.02  # ìŠ¤í… ì‚¬ì´ì¦ˆ
x_min, x_max = X_2d[:, 0].min() - 0.1, X_2d[:, 0].max() + 0.1
y_min, y_max = X_2d[:, 1].min() - 0.1, X_2d[:, 1].max() + 0.1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

# 7. ëª¨ë¸ë¡œ ì˜ˆì¸¡
Z = svm_model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# 8. ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
plt.figure(figsize=(12, 5))

# ì™¼ìª½: ê²°ì • ê²½ê³„
plt.subplot(1, 2, 1)
plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')
plt.scatter(X_2d[y==0, 0], X_2d[y==0, 1], c='white', marker='o', edgecolors='gray', s=50)
plt.scatter(X_2d[y==1, 0], X_2d[y==1, 1], c='black', marker='o', s=50)
plt.title('SVM Decision Boundary')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')

# ì˜¤ë¥¸ìª½: ëª¨ë“  íŠ¹ì„± í¬í•¨í•œ ì •ê·œí™” ê³¼ì •
plt.subplot(1, 2, 2)
plt.text(0.1, 0.9, 'Normalization Steps:', transform=plt.gca().transAxes, 
         fontsize=12, fontweight='bold')
plt.text(0.1, 0.8, f'Training samples: {len(trainX):,}', 
         transform=plt.gca().transAxes, fontsize=10)
plt.text(0.1, 0.7, f'Features: {trainX.shape[1]}', 
         transform=plt.gca().transAxes, fontsize=10)
plt.text(0.1, 0.6, f'Scale: [0, 2.0] (MinMaxScaler)', 
         transform=plt.gca().transAxes, fontsize=10)
plt.text(0.1, 0.5, f'Kernel: RBF', 
         transform=plt.gca().transAxes, fontsize=10)
plt.axis('off')

plt.tight_layout()
plt.savefig('svm_decision_boundary.png', dpi=300, bbox_inches='tight')
print('âœ… SVM Decision Boundary ì €ì¥ ì™„ë£Œ')
plt.close()
```

### âœ… ì‹¤í–‰ ë°©ë²•
```powershell
cd c:\Users\aqort\OneDrive\Desktop\gmsc
python svm_decision_boundary.py
```

### ğŸ“Š ë°œí‘œ ì‹œ í™œìš©
```
"Support Vector Machineì€ ë˜ ë‹¤ë¥¸ ê°•ë ¥í•œ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.
2D í‰ë©´ì—ì„œ ë³´ë“¯ì´ SVMì€ ë‘ í´ë˜ìŠ¤ë¥¼ ë¶„ë¦¬í•˜ëŠ” ìµœì ì˜ ì´ˆí‰ë©´ì„ ì°¾ìŠµë‹ˆë‹¤.
ìš°ë¦¬ ë°ì´í„°ì—ì„œë„ SVMê³¼ XGBoostë¥¼ ë¹„êµí•˜ë©´ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì¼ ê²ƒì…ë‹ˆë‹¤."
```

---

## ğŸ“¸ ì‚¬ì§„ 2: Logistic Regression (ë¡œì§€ìŠ¤í‹± íšŒê·€)

### ğŸ“‹ ì´ ê·¸ë˜í”„ê°€ ë­”ê°€ìš”?
- **ëª©ì **: ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ ì‹œê·¸ëª¨ì´ë“œ(Sigmoid) í•¨ìˆ˜ ì‹œê°í™”
- **ìƒë‹¨ ì™¼ìª½ (a)**: ê¸°ë³¸ ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜ (Sì ê³¡ì„ )
- **ìƒë‹¨ ì˜¤ë¥¸ìª½ (b)**: ë¯¸ë¶„ëœ í™•ë¥ ë°€ë„í•¨ìˆ˜
- **í•˜ë‹¨ ì™¼ìª½ (c)**: 2ê°œ íŠ¹ì„±ì˜ 3D ë¡œì§€ìŠ¤í‹± í‘œë©´
- **í•˜ë‹¨ ì˜¤ë¥¸ìª½ (d)**: 3ê°œ íŠ¹ì„±ì˜ 3D ë¡œì§€ìŠ¤í‹± í‘œë©´

**ìˆ˜ì‹**: $E(y_i) = \pi_i = \frac{\exp(x_i'\beta)}{1 + \exp(x_i'\beta)}$

### ğŸ” ìš°ë¦¬ í”„ë¡œì íŠ¸ì— ì ìš© ê°€ëŠ¥?
| í•­ëª© | ìƒíƒœ | ì´ìœ  |
|------|------|------|
| **í•„ìš”ì„±** | â­â­â­â­ (ë§¤ìš° ë†’ìŒ) | ìš°ë¦¬ëŠ” ì´ë¯¸ Logistic Regression ëª¨ë¸ í•™ìŠµí•¨ |
| **ì¶”ì²œ** | âœ… ëª¨ë¸ ì´ë¡  ì„¤ëª…ìš© | êµìœ¡ì  ê°€ì¹˜ ë†’ìŒ |
| **ìš°ì„ ìˆœìœ„** | 2ìˆœìœ„ | ëª¨ë¸ ì„¤ëª…ì— í•„ìˆ˜ |

### ğŸ’» ì§ì ‘ ë§Œë“œëŠ” ë°©ë²•

**íŒŒì¼ëª…**: `logistic_regression_visualization.py` ìƒì„±

```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import LogisticRegression
import pandas as pd

# 1. ë°ì´í„° ë¡œë“œ
train_data = pd.read_csv('cs-training-engineered.csv')
testX = train_data.iloc[:5000, :-1].values  # ì²˜ìŒ 5000ê°œë§Œ (ë¹ ë¥¸ ì—°ì‚°)
testY = train_data.iloc[:5000, -1].values

# 2. ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í•™ìŠµ
lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(testX, testY)

# 3. ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜ ì •ì˜
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# 4. ê·¸ë˜í”„ ìƒì„±
fig = plt.figure(figsize=(14, 10))

# (a) ê¸°ë³¸ ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜
ax1 = plt.subplot(2, 2, 1)
x = np.linspace(-6, 6, 100)
y = sigmoid(x)
ax1.plot(x, y, 'b-', linewidth=2)
ax1.set_xlabel('x')
ax1.set_ylabel('E(y)')
ax1.set_title('(a) Logistic Function: $E(y) = \\frac{1}{1+e^{-x}}$')
ax1.grid(True, alpha=0.3)

# (b) í™•ë¥ ë°€ë„í•¨ìˆ˜
ax2 = plt.subplot(2, 2, 2)
y_prime = sigmoid(x) * (1 - sigmoid(x))
ax2.plot(x, y_prime, 'r-', linewidth=2)
ax2.set_xlabel('x')
ax2.set_ylabel("E'(y)")
ax2.set_title('(b) Derivative (Probability Density)')
ax2.grid(True, alpha=0.3)

# (c) 2D íŠ¹ì„±ì˜ 3D í‘œë©´
ax3 = plt.subplot(2, 2, 3, projection='3d')
x1_range = np.linspace(-1, 1, 30)
x2_range = np.linspace(-1, 1, 30)
X1, X2 = np.meshgrid(x1_range, x2_range)
Z = sigmoid(lr_model.intercept_[0] + lr_model.coef_[0, 0]*X1 + lr_model.coef_[0, 1]*X2)
ax3.plot_surface(X1, X2, Z, cmap='viridis', alpha=0.7)
ax3.set_xlabel('xâ‚')
ax3.set_ylabel('xâ‚‚')
ax3.set_zlabel('E(y)')
ax3.set_title('(c) 2D Logistic Surface')

# (d) 3D íŠ¹ì„±ì˜ ì¶•ì†Œëœ í‘œë©´
ax4 = plt.subplot(2, 2, 4, projection='3d')
x1_range = np.linspace(-1, 1, 20)
x2_range = np.linspace(-1, 1, 20)
X1, X2 = np.meshgrid(x1_range, x2_range)
Z = sigmoid(lr_model.intercept_[0] + lr_model.coef_[0, 0]*X1 + lr_model.coef_[0, 1]*X2)
ax4.plot_surface(X1, X2, Z, cmap='plasma', alpha=0.7)
ax4.set_xlabel('xâ‚')
ax4.set_ylabel('xâ‚‚')
ax4.set_zlabel('E(y)')
ax4.set_title('(d) 3D Logistic Surface (Projected)')

plt.tight_layout()
plt.savefig('logistic_regression_visualization.png', dpi=300, bbox_inches='tight')
print('âœ… Logistic Regression ì‹œê°í™” ì €ì¥ ì™„ë£Œ')
plt.close()
```

### âœ… ì‹¤í–‰ ë°©ë²•
```powershell
cd c:\Users\aqort\OneDrive\Desktop\gmsc
python logistic_regression_visualization.py
```

### ğŸ“Š ë°œí‘œ ì‹œ í™œìš©
```
"ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” ì„ í˜• ëª¨ë¸ì…ë‹ˆë‹¤. Sì í˜•íƒœì˜ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´
ì…ë ¥ê°’ì„ [0, 1] í™•ë¥ ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì´ëŠ” ì´ì§„ ë¶„ë¥˜ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

ìš°ë¦¬ ëª¨ë¸ê³¼ì˜ ë¹„êµ:
- ë¡œì§€ìŠ¤í‹± íšŒê·€: 77.08% ì •í™•ë„, 0.8511 AUC
- XGBoost: 83.16% ì •í™•ë„, 0.8890 AUC

XGBoostê°€ ë” ë³µì¡í•œ ë¹„ì„ í˜• ê´€ê³„ë¥¼ í¬ì°©í•©ë‹ˆë‹¤."
```

---

## ğŸ“¸ ì‚¬ì§„ 3: Outlier ë¶„ì„

### ğŸ“‹ ì´ ê·¸ë˜í”„ê°€ ë­”ê°€ìš”?
- **ëª©ì **: ë‹¤ì–‘í•œ ì´ìƒì¹˜(Outlier) íƒì§€ ë°©ë²• ì‹œê°í™”
- **ì™¼ìª½ ìƒë‹¨**: Mahalanobis Distance íˆíŠ¸ë§µ
  - ë…¹ìƒ‰/ë…¸ë€ìƒ‰: ì •ìƒ ë²”ìœ„
  - ë¹¨ê°•/ë¶„í™: ì´ìƒì¹˜ ê°€ëŠ¥ì„±
- **ì™¼ìª½ í•˜ë‹¨**: ì‹¤ì œ ì´ìƒì¹˜ ë¶„í¬ (Z-score ê¸°ë°˜)
- **ì˜¤ë¥¸ìª½ ìƒë‹¨**: ì´ìƒì¹˜ ì œê±° ì „ íˆìŠ¤í† ê·¸ë¨
- **ì˜¤ë¥¸ìª½ í•˜ë‹¨**: ì´ìƒì¹˜ ì œê±° í›„ íˆìŠ¤í† ê·¸ë¨

**ì´ìƒì¹˜ íƒì§€ ë°©ë²• ë¹„êµ**:
- SVM: ì ì‘í˜•, ë¹„ì„ í˜• ì´ìƒì¹˜ íƒì§€
- BDT: ì•™ìƒë¸” ê¸°ë²•
- LR: ì”ì°¨ ê¸°ë°˜
- NN: ì‹ ê²½ë§ ê¸°ë°˜

### ğŸ” ìš°ë¦¬ í”„ë¡œì íŠ¸ì— ì ìš© ê°€ëŠ¥?
| í•­ëª© | ìƒíƒœ | ì´ìœ  |
|------|------|------|
| **í•„ìš”ì„±** | â­â­â­â­â­ (í•„ìˆ˜) | ìš°ë¦¬ê°€ ì´ë¯¸ IQRë¡œ ì´ìƒì¹˜ ì œê±°í•¨ |
| **ì¶”ì²œ** | âœ… ë°˜ë“œì‹œ í¬í•¨ | ì „ì²˜ë¦¬ ê³¼ì •ì˜ í•µì‹¬ ì¦ê±° |
| **ìš°ì„ ìˆœìœ„** | 1ìˆœìœ„ (ìµœìš°ì„ ) | ë°œí‘œì˜ ì‹ ë¢°ì„± ë³´ì¦ |

### ğŸ’» ì§ì ‘ ë§Œë“œëŠ” ë°©ë²•

**íŒŒì¼ëª…**: `outlier_analysis_advanced.py` ìƒì„±

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.spatial.distance import mahalanobis
from scipy import stats
import seaborn as sns

# 1. ë°ì´í„° ë¡œë“œ
train_before = pd.read_csv('cs-training.csv')
train_after = pd.read_csv('cs-training-preprocessed.csv')

# 2. Mahalanobis Distance ê³„ì‚°
def mahalanobis_distance(data):
    mean = np.mean(data, axis=0)
    cov = np.cov(data.T)
    inv_cov = np.linalg.inv(cov)
    distances = []
    for i in range(len(data)):
        diff = data[i] - mean
        dist = np.sqrt(diff.dot(inv_cov).dot(diff.T))
        distances.append(dist)
    return np.array(distances)

# ì‘ì€ ìƒ˜í”Œë¡œ ì‹œê°í™” (ë¹ ë¥¸ ì—°ì‚°)
data_sample = train_before.iloc[:2000, :].values
distances = mahalanobis_distance(data_sample)

# 3. ê·¸ë˜í”„ ìƒì„±
fig = plt.figure(figsize=(14, 10))

# ìƒë‹¨ ì™¼ìª½: Mahalanobis Distance íˆíŠ¸ë§µ
ax1 = plt.subplot(2, 2, 1)
# 2D ë°ì´í„°ë§Œ ì‚¬ìš© (ì‹œê°í™” ìš©ì´)
x1 = data_sample[:, 0]
x2 = data_sample[:, 1]
scatter = ax1.scatter(x1, x2, c=distances, cmap='RdYlGn_r', s=30, alpha=0.6)
plt.colorbar(scatter, ax=ax1, label='Mahalanobis Distance Value')
ax1.set_xlabel('Independent Variable 1')
ax1.set_ylabel('Independent Variable 2')
ax1.set_title('Mahalanobis Distance Visualization')

# ìƒë‹¨ ì˜¤ë¥¸ìª½: ì´ìƒì¹˜ ì œê±° ì „
ax2 = plt.subplot(2, 2, 2)
feature_before = train_before.iloc[:, 0]
ax2.hist(feature_before, bins=50, edgecolor='black', alpha=0.7)
ax2.set_title('Histogram of prop (Before Outlier Removal)')
ax2.set_xlabel('prop')
ax2.set_ylabel('Frequency')

# í•˜ë‹¨ ì™¼ìª½: Z-score ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€
ax3 = plt.subplot(2, 2, 3)
z_scores = np.abs(stats.zscore(data_sample[:, 0]))
outlier_indices = np.where(z_scores > 3)[0]
ax3.scatter(range(len(z_scores)), z_scores, alpha=0.5, s=10, label='Normal')
ax3.scatter(outlier_indices, z_scores[outlier_indices], color='red', s=50, label='Outlier (Z>3)')
ax3.axhline(y=3, color='r', linestyle='--', label='Threshold (Z=3)')
ax3.set_xlabel('Index')
ax3.set_ylabel('Z-Score')
ax3.set_title('Z-Score Based Outlier Detection')
ax3.legend()

# í•˜ë‹¨ ì˜¤ë¥¸ìª½: ì´ìƒì¹˜ ì œê±° í›„
ax4 = plt.subplot(2, 2, 4)
feature_after = train_after.iloc[:, 0]
ax4.hist(feature_after, bins=50, edgecolor='black', alpha=0.7, color='green')
ax4.set_title('Histogram of prop (After Outlier Removal)')
ax4.set_xlabel('prop')
ax4.set_ylabel('Frequency')

plt.tight_layout()
plt.savefig('outlier_analysis_advanced.png', dpi=300, bbox_inches='tight')
print('âœ… Outlier ë¶„ì„ ì‹œê°í™” ì €ì¥ ì™„ë£Œ')
plt.close()

# 5. ì´ìƒì¹˜ í†µê³„
print(f"\nğŸ“Š ì´ìƒì¹˜ ì œê±° íš¨ê³¼:")
print(f"ì œê±° ì „: {len(train_before):,} ìƒ˜í”Œ")
print(f"ì œê±° í›„: {len(train_after):,} ìƒ˜í”Œ")
print(f"ì œê±°ìœ¨: {(1 - len(train_after)/len(train_before))*100:.2f}%")
```

### âœ… ì‹¤í–‰ ë°©ë²•
```powershell
cd c:\Users\aqort\OneDrive\Desktop\gmsc
python outlier_analysis_advanced.py
```

### ğŸ“Š ë°œí‘œ ì‹œ í™œìš©
```
"ë°ì´í„° í’ˆì§ˆì€ ëª¨ë¸ ì„±ëŠ¥ì˜ í•µì‹¬ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” Mahalanobis Distanceì™€ Z-score
ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

ê²°ê³¼:
- ì´ìƒì¹˜ ê°ì§€: ì•½ 49% (78,198ê°œ ìƒ˜í”Œ ì œê±°)
- ì œê±° í›„ ë°ì´í„°: 75,167ê°œ ìœ íš¨ ìƒ˜í”Œ
- ì´ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì•ˆì •ì„±ê³¼ ì‹ ë¢°ì„±ì´ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤."
```

---

## ğŸ“¸ ì‚¬ì§„ 4: Outlier ì œê±° íš¨ê³¼ ë¹„êµ

### ğŸ“‹ ì´ ê·¸ë˜í”„ê°€ ë­”ê°€ìš”?
- **ì™¼ìª½ ìƒë‹¨**: ì´ìƒì¹˜ ì œê±° ì „ íŠ¹ì„± ë¶„í¬
- **ì™¼ìª½ í•˜ë‹¨**: ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (F1-Score)
  - íŒŒë€ìƒ‰: Logistic Regression
  - ë¹¨ê°•ìƒ‰: Neural Network
  - ì´ˆë¡ìƒ‰: BDT (Boosting Decision Tree)
  - ê²€ì •ìƒ‰: SVM
  - **ê²°ë¡ **: ì´ìƒì¹˜ ì œê±° í›„ ëª¨ë“  ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ
  
- **ì˜¤ë¥¸ìª½**: ì´ìƒì¹˜ ì œê±° í›„ íŠ¹ì„± ë¶„í¬ (ì •ê·œë¶„í¬ì— ê°€ê¹Œì›Œì§)

### ğŸ” ìš°ë¦¬ í”„ë¡œì íŠ¸ì— ì ìš© ê°€ëŠ¥?
| í•­ëª© | ìƒíƒœ | ì´ìœ  |
|------|------|------|
| **í•„ìš”ì„±** | â­â­â­â­ (ë§¤ìš° ë†’ìŒ) | ìš°ë¦¬ë„ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì´ìƒì¹˜ ì œê±° |
| **ì¶”ì²œ** | âœ… ë¹„êµ ë¶„ì„ìš© | ë‹¤ë¥¸ ëª¨ë¸ë“¤ê³¼ì˜ ì„±ëŠ¥ ë¹„êµ |
| **ìš°ì„ ìˆœìœ„** | 3ìˆœìœ„ | Outlier ë¶„ì„ í›„ ë§Œë“¤ê¸° |

### ğŸ’» ì§ì ‘ ë§Œë“œëŠ” ë°©ë²•

**íŒŒì¼ëª…**: `outlier_impact_comparison.py` ìƒì„±

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import f1_score

# 1. ë°ì´í„° ë¡œë“œ
train_before = pd.read_csv('cs-training.csv')
train_after = pd.read_csv('cs-training-preprocessed.csv')
test_before = pd.read_csv('cs-test.csv')
test_after = pd.read_csv('cs-test-preprocessed.csv')

# 2. íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
X_before = train_before.iloc[:, :-1].values
y_before = train_before.iloc[:, -1].values
X_after = train_after.iloc[:, :-1].values
y_after = train_after.iloc[:, -1].values

X_test_before = test_before.iloc[:, :-1].values
y_test_before = test_before.iloc[:, -1].values
X_test_after = test_after.iloc[:, :-1].values
y_test_after = test_after.iloc[:, -1].values

# 3. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Random Forest': RandomForestClassifier(n_estimators=100),
    'SVM': SVC(kernel='rbf')
}

f1_scores_before = []
f1_scores_after = []

print("ğŸ”„ ëª¨ë¸ í•™ìŠµ ì¤‘...\n")
for name, model in models.items():
    # ì´ìƒì¹˜ ì œê±° ì „
    model.fit(X_before, y_before)
    y_pred_before = model.predict(X_test_before)
    f1_before = f1_score(y_test_before, y_pred_before)
    f1_scores_before.append(f1_before)
    
    # ì´ìƒì¹˜ ì œê±° í›„
    model.fit(X_after, y_after)
    y_pred_after = model.predict(X_test_after)
    f1_after = f1_score(y_test_after, y_pred_after)
    f1_scores_after.append(f1_after)
    
    print(f"{name}:")
    print(f"  ì œê±° ì „ F1-Score: {f1_before:.4f}")
    print(f"  ì œê±° í›„ F1-Score: {f1_after:.4f}")
    print(f"  í–¥ìƒë„: +{(f1_after-f1_before)*100:.2f}%\n")

# 4. ê·¸ë˜í”„ ìƒì„±
fig = plt.figure(figsize=(14, 6))

# ì™¼ìª½ ìƒë‹¨: ì´ìƒì¹˜ ì œê±° ì „ ë¶„í¬
ax1 = plt.subplot(1, 2, 1)
feature_idx = 0
ax1.hist(X_before[:, feature_idx], bins=50, edgecolor='black', alpha=0.7)
ax1.set_title('Feature Distribution\n(Before Outlier Removal)')
ax1.set_xlabel('Feature Value')
ax1.set_ylabel('Frequency')

# ì˜¤ë¥¸ìª½: ì´ìƒì¹˜ ì œê±° í›„ ë¶„í¬
ax2 = plt.subplot(1, 2, 2)
ax2.hist(X_after[:, feature_idx], bins=50, edgecolor='black', alpha=0.7, color='green')
ax2.set_title('Feature Distribution\n(After Outlier Removal)')
ax2.set_xlabel('Feature Value')
ax2.set_ylabel('Frequency')

plt.tight_layout()
plt.savefig('outlier_impact_comparison.png', dpi=300, bbox_inches='tight')
print('âœ… Outlier ì œê±° íš¨ê³¼ ë¹„êµ ì €ì¥ ì™„ë£Œ')

# 5. ë§‰ëŒ€ ê·¸ë˜í”„: F1-Score ë¹„êµ
fig, ax = plt.subplots(figsize=(10, 6))
x = np.arange(len(models))
width = 0.35

bars1 = ax.bar(x - width/2, f1_scores_before, width, label='Before Removal', alpha=0.8)
bars2 = ax.bar(x + width/2, f1_scores_after, width, label='After Removal', alpha=0.8, color='green')

ax.set_ylabel('F1-Score')
ax.set_title('Model Performance: Impact of Outlier Removal')
ax.set_xticks(x)
ax.set_xticklabels(models.keys())
ax.legend()
ax.set_ylim([0, 1])

# ê°’ ë¼ë²¨ ì¶”ê°€
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.4f}',
                ha='center', va='bottom', fontsize=10)

plt.tight_layout()
plt.savefig('model_performance_comparison.png', dpi=300, bbox_inches='tight')
print('âœ… ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì €ì¥ ì™„ë£Œ')
plt.close()
```

### âœ… ì‹¤í–‰ ë°©ë²•
```powershell
cd c:\Users\aqort\OneDrive\Desktop\gmsc
python outlier_impact_comparison.py
```

---

## ğŸ“Š **ìµœì¢… ì •ë¦¬: ì–´ë–¤ ê·¸ë˜í”„ê°€ ë°œí‘œì— í•„ìš”í•œê°€?**

### ğŸ¯ **í•„ìˆ˜ ê·¸ë˜í”„ (ë°˜ë“œì‹œ í¬í•¨)**

| ìˆœìœ„ | ê·¸ë˜í”„ | íŒŒì¼ëª… | ì¤‘ìš”ë„ | ì´ìœ  |
|------|--------|--------|--------|------|
| **1ìˆœìœ„** | Outlier ë¶„ì„ | `outlier_analysis_advanced.py` | â­â­â­â­â­ | ë°ì´í„° í’ˆì§ˆ ì¦ëª… |
| **2ìˆœìœ„** | Logistic Regression ì´ë¡  | `logistic_regression_visualization.py` | â­â­â­â­ | ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ ì„¤ëª… |
| **3ìˆœìœ„** | Outlier ì œê±° íš¨ê³¼ | `outlier_impact_comparison.py` | â­â­â­â­ | ì „ì²˜ë¦¬ íš¨ê³¼ ì…ì¦ |
| **4ìˆœìœ„** | SVM ê²°ì • ê²½ê³„ | `svm_decision_boundary.py` | â­â­â­ | ëª¨ë¸ ë‹¤ì–‘ì„± |

### âœ… **í˜„ì¬ ìš°ë¦¬ê°€ ì´ë¯¸ ê°€ì§„ ê²ƒ**

```
âœ“ 07_confusion_matrix.png (XGBoost í˜¼ë™í–‰ë ¬)
âœ“ 08_roc_curves.png (3ê°œ ëª¨ë¸ ROC ë¹„êµ)
âœ“ 09_precision_recall_curve.png (Precision-Recall)
âœ“ 10_feature_importance.png (íŠ¹ì„± ì¤‘ìš”ë„)
âœ“ 11_radar_chart.png (ëª¨ë¸ ë©”íŠ¸ë¦­ ë¹„êµ)
âœ“ 12_correlation_heatmap.png (ìƒê´€ê³„ìˆ˜)
```

### ğŸ†• **ì¶”ê°€ë¡œ ë§Œë“¤ë©´ ì¢‹ì€ ê²ƒ**

| # | ê·¸ë˜í”„ | ë°œí‘œ ì„¹ì…˜ | ê°•ë„ |
|---|--------|---------|------|
| 1 | Outlier ë¶„ì„ | "ì „ì²˜ë¦¬" | ë§¤ìš° ì¤‘ìš” |
| 2 | Logistic Regression ì‹œê°í™” | "ëª¨ë¸ ì´ë¡ " | ì¤‘ìš” |
| 3 | Outlier ì œê±° íš¨ê³¼ | "ì „ì²˜ë¦¬ ê²€ì¦" | ì¤‘ìš” |
| 4 | SVM ê²°ì • ê²½ê³„ | "ëª¨ë¸ ë¹„êµ" | ì„ íƒ |

---

## ğŸ¬ **ë°œí‘œ ìŠ¬ë¼ì´ë“œ êµ¬ì„± ì˜ˆì‹œ**

### **ìŠ¬ë¼ì´ë“œ 1-3: ë°ì´í„° ì „ì²˜ë¦¬**
```
ìŠ¬ë¼ì´ë“œ 1: ì›ë³¸ ë°ì´í„° ê°œìš”
ìŠ¬ë¼ì´ë“œ 2: Outlier ë¶„ì„ (NEW - outlier_analysis_advanced.png)
ìŠ¬ë¼ì´ë“œ 3: Outlier ì œê±° íš¨ê³¼ (NEW - outlier_impact_comparison.png)
```

### **ìŠ¬ë¼ì´ë“œ 4-6: ëª¨ë¸ ì´ë¡ **
```
ìŠ¬ë¼ì´ë“œ 4: Logistic Regression ê°œë¡  (NEW - logistic_regression_visualization.py)
ìŠ¬ë¼ì´ë“œ 5: SVM ê°œë¡  (NEW - svm_decision_boundary.py)
ìŠ¬ë¼ì´ë“œ 6: XGBoost ê°œë¡  (ê¸°ì¡´ ìë£Œ)
```

### **ìŠ¬ë¼ì´ë“œ 7-12: ëª¨ë¸ ì„±ëŠ¥**
```
ìŠ¬ë¼ì´ë“œ 7: Confusion Matrix (07_confusion_matrix.png)
ìŠ¬ë¼ì´ë“œ 8: ROC Curves (08_roc_curves.png)
ìŠ¬ë¼ì´ë“œ 9: Precision-Recall (09_precision_recall_curve.png)
ìŠ¬ë¼ì´ë“œ 10: Feature Importance (10_feature_importance.png)
ìŠ¬ë¼ì´ë“œ 11: ëª¨ë¸ ë¹„êµ (11_radar_chart.png)
ìŠ¬ë¼ì´ë“œ 12: ìƒê´€ ë¶„ì„ (12_correlation_heatmap.png)
```

---

## ğŸ“ **ì²´í¬ë¦¬ìŠ¤íŠ¸: ê° ê·¸ë˜í”„ ì‹¤í–‰ ìˆœì„œ**

```
[ ] 1. outlier_analysis_advanced.py ì‹¤í–‰
      â†’ outlier_analysis_advanced.png ìƒì„±

[ ] 2. outlier_impact_comparison.py ì‹¤í–‰
      â†’ outlier_impact_comparison.png
      â†’ model_performance_comparison.png ìƒì„±

[ ] 3. logistic_regression_visualization.py ì‹¤í–‰
      â†’ logistic_regression_visualization.png ìƒì„±

[ ] 4. svm_decision_boundary.py ì‹¤í–‰
      â†’ svm_decision_boundary.png ìƒì„±

[ ] 5. ëª¨ë“  ê·¸ë˜í”„ë¥¼ PPTì— ë°°ì¹˜
      â†’ ì™„ì„±! ğŸ‰
```

---

## ğŸ“ **ìµœì¢… ì¶”ì²œ**

**ì‹œê°„ì´ ì—†ë‹¤ë©´ (1ìˆœìœ„ë§Œ ì„ íƒ):**
- âœ… Outlier ë¶„ì„ (`outlier_analysis_advanced.py`)

**ì‹œê°„ì´ ì¶©ë¶„í•˜ë‹¤ë©´ (ëª¨ë‘ ì„ íƒ):**
- âœ… Outlier ë¶„ì„
- âœ… Logistic Regression ì‹œê°í™”
- âœ… Outlier ì œê±° íš¨ê³¼
- âœ… SVM ê²°ì • ê²½ê³„

**ì¢‹ìœ¼ë©´ ì¢‹ì„ìˆ˜ë¡ (ëª¨ë“  ê³ ê¸‰ ê·¸ë˜í”„):**
- âœ… ìœ„ì˜ ëª¨ë“  ê²ƒ
- âœ… ê¸°ì¡´ì˜ 12ê°œ ì‹œê°í™”

---

ì´ì œ ê° ê·¸ë˜í”„ë¥¼ ì§ì ‘ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤!
ì–´ëŠ ê²ƒë¶€í„° ì‹œì‘í• ê¹Œìš”?
