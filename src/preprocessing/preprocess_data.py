import csv
import statistics
from collections import defaultdict

# 1. ë°ì´í„° ì½ê¸°
def load_data(filename):
    with open(filename, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        header = next(reader)
        data = [list(map(float, row)) for row in reader]
    return header, data

# 2. ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ê²°ì¸¡ì¹˜ ì—†ëŠ”ì§€ í™•ì¸)
def check_missing_values(data, header):
    print("=" * 60)
    print("1ï¸âƒ£  ê²°ì¸¡ì¹˜ ê²€ì‚¬")
    print("=" * 60)
    missing_count = defaultdict(int)
    for row in data:
        for i, val in enumerate(row):
            if val is None or (isinstance(val, str) and val.strip() == ''):
                missing_count[header[i]] += 1
    
    if missing_count:
        print("âŒ ê²°ì¸¡ì¹˜ ë°œê²¬:")
        for col, count in missing_count.items():
            print(f"  {col}: {count}ê°œ")
    else:
        print("âœ… ê²°ì¸¡ì¹˜ ì—†ìŒ")
    return len(missing_count) == 0

# 3. ì´ìƒì¹˜ íƒì§€ ë° ì œê±° (IQR ë°©ì‹)
def remove_outliers(data, header):
    print("\n" + "=" * 60)
    print("2ï¸âƒ£  ì´ìƒì¹˜ íƒì§€ ë° ì œê±° (IQR ë°©ì‹)")
    print("=" * 60)
    
    before_count = len(data)
    outlier_indices = set()
    
    # ê° ì»¬ëŸ¼ë³„ë¡œ ì´ìƒì¹˜ íƒì§€
    for col_idx in range(len(header)):
        values = [row[col_idx] for row in data if row[col_idx] is not None]
        
        if len(values) > 0:
            values_sorted = sorted(values)
            q1_idx = len(values_sorted) // 4
            q3_idx = 3 * len(values_sorted) // 4
            q1 = values_sorted[q1_idx]
            q3 = values_sorted[q3_idx]
            iqr = q3 - q1
            
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr
            
            for row_idx, row in enumerate(data):
                val = row[col_idx]
                if val < lower_bound or val > upper_bound:
                    outlier_indices.add(row_idx)
    
    # ì´ìƒì¹˜ í–‰ ì œê±°
    data_cleaned = [row for idx, row in enumerate(data) if idx not in outlier_indices]
    after_count = len(data_cleaned)
    removed = before_count - after_count
    
    print(f"ì´ì „ í–‰ ìˆ˜: {before_count}")
    print(f"ì´ìƒì¹˜ ì œê±°: {removed}ê°œ")
    print(f"ì´í›„ í–‰ ìˆ˜: {after_count}")
    print(f"ì œê±°ìœ¨: {removed/before_count*100:.2f}%")
    
    return data_cleaned

# 4. ê¸°ë³¸ í†µê³„ ë¶„ì„
def print_statistics(data, header):
    print("\n" + "=" * 60)
    print("3ï¸âƒ£  ë°ì´í„° í†µê³„")
    print("=" * 60)
    print(f"ì´ ìƒ˜í”Œ: {len(data)}")
    print(f"ì´ íŠ¹ì„±: {len(header)}")
    print(f"\nì»¬ëŸ¼ë³„ í†µê³„:")
    print("-" * 60)
    
    for col_idx, col_name in enumerate(header):
        values = [row[col_idx] for row in data]
        if len(values) > 0:
            mean_val = statistics.mean(values)
            min_val = min(values)
            max_val = max(values)
            median_val = statistics.median(values)
            try:
                std_val = statistics.stdev(values)
            except:
                std_val = 0
            
            print(f"\n{col_name}:")
            print(f"  í‰ê· : {mean_val:.4f}")
            print(f"  ì¤‘ì•™ê°’: {median_val:.4f}")
            print(f"  í‘œì¤€í¸ì°¨: {std_val:.4f}")
            print(f"  ìµœì†Œ: {min_val:.4f}")
            print(f"  ìµœëŒ€: {max_val:.4f}")

# 5. ì •ê·œí™” (0-1 ë²”ìœ„)
def normalize_data(data, header):
    print("\n" + "=" * 60)
    print("4ï¸âƒ£  ë°ì´í„° ì •ê·œí™” (0-1 ë²”ìœ„)")
    print("=" * 60)
    
    normalized_data = []
    min_max = []
    
    # ê° ì»¬ëŸ¼ì˜ ìµœì†Œ/ìµœëŒ€ ê³„ì‚°
    for col_idx in range(len(header)):
        values = [row[col_idx] for row in data]
        min_val = min(values)
        max_val = max(values)
        min_max.append((min_val, max_val))
    
    # ì •ê·œí™” ì ìš©
    for row in data:
        normalized_row = []
        for col_idx, val in enumerate(row):
            min_val, max_val = min_max[col_idx]
            if max_val - min_val == 0:
                normalized_val = 0
            else:
                normalized_val = (val - min_val) / (max_val - min_val)
            normalized_row.append(normalized_val)
        normalized_data.append(normalized_row)
    
    print("âœ… ì •ê·œí™” ì™„ë£Œ (ëª¨ë“  ê°’ì´ 0-1 ë²”ìœ„ë¡œ ë³€í™˜)")
    return normalized_data

# 6. ë°ì´í„° ì €ì¥
def save_data(data, header, filename):
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(header)
        writer.writerows(data)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {filename}")

# ë©”ì¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
def main():
    print("\n")
    print("â•”" + "=" * 58 + "â•—")
    print("â•‘" + " " * 15 + "ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸" + " " * 17 + "â•‘")
    print("â•š" + "=" * 58 + "â•")
    
    # Training ë°ì´í„° ì „ì²˜ë¦¬
    print("\n\nğŸ“Š cs-training.csv ì „ì²˜ë¦¬ ì¤‘...")
    header_train, data_train = load_data('cs-training.csv')
    
    check_missing_values(data_train, header_train)
    data_train = remove_outliers(data_train, header_train)
    print_statistics(data_train, header_train)
    data_train_normalized = normalize_data(data_train, header_train)
    save_data(data_train_normalized, header_train, 'cs-training-preprocessed.csv')
    
    # Test ë°ì´í„° ì „ì²˜ë¦¬
    print("\n\nğŸ“Š cs-test.csv ì „ì²˜ë¦¬ ì¤‘...")
    header_test, data_test = load_data('cs-test.csv')
    
    check_missing_values(data_test, header_test)
    data_test = remove_outliers(data_test, header_test)
    print_statistics(data_test, header_test)
    data_test_normalized = normalize_data(data_test, header_test)
    save_data(data_test_normalized, header_test, 'cs-test-preprocessed.csv')
    
    print("\n\n" + "=" * 60)
    print("âœ¨ ëª¨ë“  ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print("=" * 60)
    print(f"Training: ì›ë³¸ {len(data_train)} â†’ ì •ê·œí™” ì™„ë£Œ")
    print(f"Test: ì›ë³¸ {len(data_test)} â†’ ì •ê·œí™” ì™„ë£Œ")
    print("\nìƒì„±ëœ íŒŒì¼:")
    print("  - cs-training-preprocessed.csv")
    print("  - cs-test-preprocessed.csv")

if __name__ == '__main__':
    main()
